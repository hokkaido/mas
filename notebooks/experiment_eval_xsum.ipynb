{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from spacy_readability import Readability\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import textacy\n",
    "import pickle\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "read = Readability()\n",
    "nlp.add_pipe(read, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'data/exp2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_interval(a):\n",
    "    return np.mean(a), st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(doc, n=1):\n",
    "    return textacy.extract.ngrams(doc, n, filter_stops=True, filter_punct=True, filter_nums=False)\n",
    "\n",
    "class NGrams:\n",
    "    def __init__(self, corpus=None, ns=[1, 2, 3]):\n",
    "        self.ns = ns\n",
    "        self.ngrams = defaultdict(list)\n",
    "        self.unique = defaultdict(list)\n",
    "        if corpus is not None:\n",
    "            self.load_corpus(corpus)\n",
    "            \n",
    "    def load_corpus(self, corpus):\n",
    "        for doc in corpus:\n",
    "            self.add_doc(doc)\n",
    "            \n",
    "    def add_doc(self, doc):\n",
    "        for n in self.ns:\n",
    "            ngrams = list(str(s) for s in extract_ngrams(doc, n))\n",
    "            self.ngrams[n].append(ngrams)\n",
    "            if len(ngrams) == 0:\n",
    "                continue\n",
    "            unique = set(ngrams)\n",
    "            self.unique[n].append(len(unique)/len(ngrams))\n",
    "    \n",
    "class NGramStats:\n",
    "    def __init__(self, corpus=None, ns=[1, 2, 3]):\n",
    "        self.ngrams = NGrams(corpus, ns)\n",
    "        self.overlaps = {}\n",
    "        self.raw = defaultdict(list)\n",
    "           \n",
    "    def add_doc(self, doc):\n",
    "        self.ngrams.add_doc(doc)\n",
    "           \n",
    "    def calc_overlaps(self, other_name, other_ngrams):\n",
    "        assert len(self.ngrams.ns) == len(other_ngrams.ns)\n",
    "        \n",
    "        if other_name not in self.overlaps:\n",
    "            self.overlaps[other_name] = defaultdict(list)\n",
    "\n",
    "        for n in self.ngrams.ns:\n",
    "            overlaps = self.calc_overlap(self.ngrams.ngrams[n], other_ngrams.ngrams[n])\n",
    "            self.overlaps[other_name][n] = overlaps\n",
    "    \n",
    "    def calc_overlap(self, ngram, other_ngram):\n",
    "        \n",
    "        assert len(ngram) == len(other_ngram)\n",
    "        \n",
    "        perc_overlaps = []\n",
    "\n",
    "        for i in range(len(ngram)):\n",
    "            this = ngram[i]\n",
    "            other = other_ngram[i]\n",
    "            \n",
    "            common = set(this) & set(other)\n",
    "            \n",
    "            if len(this) == 0:\n",
    "                perc_overlaps.append(0)\n",
    "                continue\n",
    "                \n",
    "            if common == 0:\n",
    "                perc_overlaps.append(0)\n",
    "                continue\n",
    "                \n",
    "            overlaps = 0\n",
    "            for j in this:\n",
    "                if str(j) in common:\n",
    "                    overlaps += 1\n",
    "            perc_overlaps.append(overlaps / len(this))\n",
    "\n",
    "        return perc_overlaps\n",
    "    \n",
    "    def stats(self):\n",
    "        results = {'overlaps': {}, 'distinct': {}}\n",
    "        \n",
    "        for n in self.ngrams.ns:\n",
    "            results['distinct'][n] = conf_interval(self.ngrams.unique[n])\n",
    "            \n",
    "        for name in self.overlaps:\n",
    "            results['overlaps'][name] = dict()\n",
    "            for n in self.overlaps[name]:\n",
    "                conf = conf_interval(self.overlaps[name][n])\n",
    "                results['overlaps'][name][n] = conf\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStats:\n",
    "    def __init__(self, text_path):\n",
    "        \n",
    "        self.general = GeneralStats()\n",
    "        self.readability = ReadabilityStats()\n",
    "        self.ngrams = NGramStats()\n",
    "        \n",
    "        self.load_corpus(text_path)\n",
    "        \n",
    "    def load_corpus(self, text_path, max_len=-1):\n",
    "        self.corpus = []\n",
    "        \n",
    "        with open(text_path,'r') as f:\n",
    "            self.texts = f.read().splitlines()[:max_len]\n",
    "            i = 0\n",
    "            for doc in nlp.pipe(self.texts):\n",
    "                if i % 1000 == 0:\n",
    "                    logger.info('{} docs loaded'.format(i))\n",
    "                self.corpus.append(doc)\n",
    "                self.readability.add_doc(doc)\n",
    "                self.general.add_doc(doc)\n",
    "                self.ngrams.add_doc(doc)\n",
    "                i+=1\n",
    "                \n",
    "    def stats(self):\n",
    "        return {\n",
    "            'readability': self.readability.stats(),\n",
    "            'general': self.general.stats(),\n",
    "            'ngrams': self.ngrams.stats()\n",
    "        }\n",
    "                \n",
    "class GeneralStats:\n",
    "    def __init__(self, corpus=None):\n",
    "        self.metrics = [\n",
    "            'n_sents', \n",
    "            'n_words', \n",
    "        ]\n",
    "        self.raw = defaultdict(list)\n",
    "        if corpus is not None:\n",
    "            self.add_corpus(corpus)\n",
    "        \n",
    "    def add_corpus(self, corpus):\n",
    "        for doc in corpus:\n",
    "            self.add_doc(doc)\n",
    "                    \n",
    "    def add_doc(self, doc):\n",
    "        self.raw['n_sents'].append(len(list(doc.sents)))\n",
    "        self.raw['n_words'].append(len(doc))\n",
    "\n",
    "    def stats(self):\n",
    "        results = dict()\n",
    "        for metric in self.metrics:\n",
    "            results[metric] = conf_interval(self.raw[metric])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "class ReadabilityStats:\n",
    "    def __init__(self, corpus=None):\n",
    "        self.metrics = [\n",
    "            'flesch_kincaid_grade_level', \n",
    "            'flesch_kincaid_reading_ease', \n",
    "            'dale_chall', \n",
    "            'coleman_liau_index', \n",
    "            'automated_readability_index']\n",
    "        \n",
    "        self.raw = defaultdict(list)\n",
    "        if corpus is not None:\n",
    "            self.add_corpus(corpus)\n",
    "        \n",
    "    def add_corpus(self, corpus):\n",
    "        for doc in corpus:\n",
    "            self.add_doc(doc)\n",
    "                    \n",
    "    def add_doc(self, doc):\n",
    "        for metric in self.metrics:\n",
    "            if not getattr(doc._, metric):\n",
    "                print('{} not found'.format(metric))\n",
    "            self.raw[metric].append(getattr(doc._, metric))\n",
    "        \n",
    "    def stats(self):\n",
    "        results = dict()\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            results[metric] = conf_interval(self.raw[metric])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentResult:\n",
    "    def __init__(self, res_path):\n",
    "        self.res_path = res_path\n",
    "        self.name = os.path.basename(res_path)\n",
    "        # Load Hypothesis\n",
    "        self.text_stats = TextStats(os.path.join(self.res_path, 'hyp.txt'))\n",
    "        \n",
    "        # Load Rouge\n",
    "        # Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoldSummaries:\n",
    "    def __init__(self, res_path):\n",
    "        self.res_path = res_path\n",
    "        \n",
    "        # Load Gold\n",
    "        self.text_stats = TextStats(os.path.join(self.res_path, 'tar.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceArticles:\n",
    "    def __init__(self, res_path):\n",
    "        self.res_path = res_path\n",
    "        \n",
    "        # Load Gold\n",
    "        self.text_stats = TextStats(os.path.join(self.res_path, 'src.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, exp_path):\n",
    "        self.exp_path = exp_path\n",
    "        self.load_experiment()\n",
    "        \n",
    "    def load_experiment(self):\n",
    "        self.results = []\n",
    "        logging.info('Loading source articles')\n",
    "        self.source = self.source = SourceArticles(self.exp_path)\n",
    "        \n",
    "        self.gold = None\n",
    "        \n",
    "        for d in os.listdir(self.exp_path):\n",
    "            res_path = os.path.join(self.exp_path, d)\n",
    "            if os.path.isdir(res_path):\n",
    "                if self.gold is None:\n",
    "                    logging.info('Loading gold summaries')\n",
    "                    self.gold = GoldSummaries(res_path)\n",
    "                    self.gold.text_stats.ngrams.calc_overlaps('source', self.source.text_stats.ngrams.ngrams)\n",
    "                logging.info('Loading {}'.format(res_path))\n",
    "                r = ExperimentResult(res_path)\n",
    "                r.text_stats.ngrams.calc_overlaps('source', self.source.text_stats.ngrams.ngrams)\n",
    "                self.results.append(r)\n",
    "                \n",
    "    def stats(self):\n",
    "        agg = {}\n",
    "        \n",
    "        agg['gold'] = self.gold.text_stats.stats()\n",
    "        \n",
    "        for r in self.results:\n",
    "            agg[r.name] = r.text_stats.stats()\n",
    "            \n",
    "        return agg\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as handle:\n",
    "            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatexWriter:\n",
    "    def __init__(self, stats, order=None):\n",
    "        self.order = order\n",
    "        self.stats = stats\n",
    "        \n",
    "    def ngram_distinct(self):\n",
    "        def write_row(model, label):\n",
    "            s = self.stats[model]['ngrams']['distinct']\n",
    "            \n",
    "            return '{} & {:0.2f} & {:0.2f} & {:0.2f} \\\\\\\\'.format(label, \n",
    "                                                    s[1][0], \n",
    "                                                    s[2][0], \n",
    "                                                    s[3][0])\n",
    "        for model, label in self.order:\n",
    "            print(write_row(model, label))\n",
    "            \n",
    "    def ngram_similarity(self):\n",
    "        def write_row(model, label):\n",
    "            s = self.stats[model]['ngrams']['overlaps']['source']\n",
    "            \n",
    "            return '{} & {:0.2f} & {:0.2f} & {:0.2f} \\\\\\\\'.format(label, \n",
    "                                                    s[1][0], \n",
    "                                                    s[2][0], \n",
    "                                                    s[3][0])\n",
    "        for model, label in self.order:\n",
    "            print(write_row(model, label))\n",
    "            \n",
    "    def length(self):\n",
    "        def write_row(model, label):\n",
    "            s = self.stats[model]['general']\n",
    "            \n",
    "            return '{} & {:0.2f} \\\\\\\\'.format(label, \n",
    "                                                    s['n_words'][0])\n",
    "        for model, label in self.order:\n",
    "            print(write_row(model, label))\n",
    "        \n",
    "    def readability(self):\n",
    "        def write_row(model, label):\n",
    "            s = self.stats[model]['readability']\n",
    "            \n",
    "            return '{} & {:0.2f} & {:0.2f} & {:0.2f} & {:0.2f} \\\\\\\\'.format(label, \n",
    "                                                    s['flesch_kincaid_grade_level'][0], \n",
    "                                                    s['flesch_kincaid_reading_ease'][0], \n",
    "                                                    s['dale_chall'][0], \n",
    "                                                    s['automated_readability_index'][0])\n",
    "        for model, label in self.order:\n",
    "            print(write_row(model, label))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading source articles\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n",
      "INFO:root:Loading gold summaries\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n",
      "INFO:root:Loading data/exp2/xsum-entities-encoder-segments-encoder\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n",
      "INFO:root:Loading data/exp2/xsum-vanilla\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n",
      "INFO:root:Loading data/exp2/xsum-segments-encoder\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n",
      "INFO:root:Loading data/exp2/xsum-entities-encoder\n",
      "INFO:root:0 docs loaded\n",
      "INFO:root:1000 docs loaded\n",
      "INFO:root:2000 docs loaded\n",
      "INFO:root:3000 docs loaded\n",
      "INFO:root:4000 docs loaded\n",
      "INFO:root:5000 docs loaded\n",
      "INFO:root:6000 docs loaded\n",
      "INFO:root:7000 docs loaded\n",
      "INFO:root:8000 docs loaded\n",
      "INFO:root:9000 docs loaded\n",
      "INFO:root:10000 docs loaded\n",
      "INFO:root:11000 docs loaded\n"
     ]
    }
   ],
   "source": [
    "exp2 = Experiment('data/exp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.save('exp2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle \n",
    "def load_pickle_gc(f):\n",
    "    output = open(f, 'rb')\n",
    "\n",
    "    # disable garbage collector\n",
    "    gc.disable()\n",
    "\n",
    "    mydict = pickle.load(output)\n",
    "\n",
    "    # enable garbage collector again\n",
    "    gc.enable()\n",
    "    output.close()\n",
    "    return mydict\n",
    "exp2 = load_pickle_gc('exp2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fsantschi/anaconda3/envs/nlp/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1920: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/Users/fsantschi/anaconda3/envs/nlp/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1921: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    }
   ],
   "source": [
    "ltx_writer = LatexWriter(exp2.stats(), \n",
    "                         order=[('gold', 'Gold summaries'),\n",
    "                                ('xsum-vanilla', 'MASS reproduced'),\n",
    "                                ('xsum-entities-encoder', 'NER-Enc'),\n",
    "                                ('xsum-segments-encoder', 'SEG-Enc'),\n",
    "                                ('xsum-entities-encoder-segments-encoder', 'NER-Enc, SEG-Enc')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp2.results[3].text_stats.ngrams.ngrams.unique[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold summaries & 10.96 & 54.01 & 10.07 & 11.48 \\\\\n",
      "MASS reproduced & 9.66 & 60.67 & 9.53 & 9.53 \\\\\n",
      "NER-Enc & 9.73 & 60.27 & 9.56 & 9.74 \\\\\n",
      "SEG-Enc & 9.74 & 60.01 & 9.61 & 9.71 \\\\\n",
      "NER-Enc, SEG-Enc & 9.88 & 59.54 & 9.64 & 9.87 \\\\\n"
     ]
    }
   ],
   "source": [
    "ltx_writer.readability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold summaries & 0.99 & 1.00 & 1.00 \\\\\n",
      "MASS reproduced & 0.97 & 1.00 & 1.00 \\\\\n",
      "NER-Enc & 0.97 & 1.00 & 1.00 \\\\\n",
      "SEG-Enc & 0.97 & 1.00 & 1.00 \\\\\n",
      "NER-Enc, SEG-Enc & 0.97 & 1.00 & 1.00 \\\\\n"
     ]
    }
   ],
   "source": [
    "ltx_writer.ngram_distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold summaries & 0.50 & 0.16 & 0.04 \\\\\n",
      "MASS reproduced & 0.63 & 0.28 & 0.08 \\\\\n",
      "NER-Enc & 0.62 & 0.28 & 0.08 \\\\\n",
      "SEG-Enc & 0.63 & 0.29 & 0.08 \\\\\n",
      "NER-Enc, SEG-Enc & 0.62 & 0.27 & 0.07 \\\\\n"
     ]
    }
   ],
   "source": [
    "ltx_writer.ngram_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold summaries & 24.34 \\\\\n",
      "MASS reproduced & 22.70 \\\\\n",
      "NER-Enc & 22.98 \\\\\n",
      "SEG-Enc & 22.72 \\\\\n",
      "NER-Enc, SEG-Enc & 23.06 \\\\\n"
     ]
    }
   ],
   "source": [
    "ltx_writer.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
